{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5ebd439-b1ce-4e18-929c-41bcca02849a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/torch_tutorial/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarkalsa\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20230213_012012-mzyimzjq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/markalsa/phase2_semantic_segmentation_initial_run/runs/mzyimzjq' target=\"_blank\">rich-surf-2</a></strong> to <a href='https://wandb.ai/markalsa/phase2_semantic_segmentation_initial_run' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/markalsa/phase2_semantic_segmentation_initial_run' target=\"_blank\">https://wandb.ai/markalsa/phase2_semantic_segmentation_initial_run</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/markalsa/phase2_semantic_segmentation_initial_run/runs/mzyimzjq' target=\"_blank\">https://wandb.ai/markalsa/phase2_semantic_segmentation_initial_run/runs/mzyimzjq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name    | Type             | Params\n",
      "---------------------------------------------\n",
      "0 | loss_fn | CrossEntropyLoss | 0     \n",
      "1 | model   | FPN              | 5.8 M \n",
      "---------------------------------------------\n",
      "5.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.8 M     Total params\n",
      "23.050    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:02<00:02,  2.86s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/torch_tutorial/lib/python3.10/site-packages/segmentation_models_pytorch/base/modules.py:116: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.activation(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/torch_tutorial/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1600: PossibleUserWarning: The number of training batches (20) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  80%|████████  | 20/25 [02:54<00:43,  8.70s/it, loss=2.09, v_num=mzjq]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  84%|████████▍ | 21/25 [03:18<00:37,  9.46s/it, loss=2.09, v_num=mzjq]\n",
      "Epoch 0:  88%|████████▊ | 22/25 [03:20<00:27,  9.13s/it, loss=2.09, v_num=mzjq]\n",
      "Epoch 0:  92%|█████████▏| 23/25 [03:23<00:17,  8.84s/it, loss=2.09, v_num=mzjq]\n",
      "Epoch 0:  96%|█████████▌| 24/25 [03:25<00:08,  8.56s/it, loss=2.09, v_num=mzjq]\n",
      "Epoch 0: 100%|██████████| 25/25 [03:27<00:00,  8.32s/it, loss=2.09, v_num=mzjq]\n",
      "Epoch 1:  80%|████████  | 20/25 [03:10<00:47,  9.51s/it, loss=2.03, v_num=mzjq]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  84%|████████▍ | 21/25 [03:44<00:42, 10.67s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 1:  88%|████████▊ | 22/25 [03:46<00:30, 10.29s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 1:  92%|█████████▏| 23/25 [03:48<00:19,  9.95s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 1:  96%|█████████▌| 24/25 [03:51<00:09,  9.63s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 1: 100%|██████████| 25/25 [03:53<00:00,  9.33s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 2:  80%|████████  | 20/25 [03:15<00:48,  9.79s/it, loss=2.03, v_num=mzjq]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  84%|████████▍ | 21/25 [03:51<00:44, 11.00s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 2:  88%|████████▊ | 22/25 [03:53<00:31, 10.61s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 2:  92%|█████████▏| 23/25 [03:55<00:20, 10.26s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 2:  96%|█████████▌| 24/25 [03:58<00:09,  9.93s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 2: 100%|██████████| 25/25 [04:00<00:00,  9.63s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 3:  80%|████████  | 20/25 [03:15<00:48,  9.76s/it, loss=2.03, v_num=mzjq]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  84%|████████▍ | 21/25 [03:50<00:43, 10.97s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 3:  88%|████████▊ | 22/25 [03:52<00:31, 10.58s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 3:  92%|█████████▏| 23/25 [03:55<00:20, 10.22s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 3:  96%|█████████▌| 24/25 [03:57<00:09,  9.90s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 3: 100%|██████████| 25/25 [03:59<00:00,  9.60s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 4:  80%|████████  | 20/25 [03:39<00:54, 10.98s/it, loss=2.03, v_num=mzjq]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  84%|████████▍ | 21/25 [04:15<00:48, 12.17s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 4:  88%|████████▊ | 22/25 [04:17<00:35, 11.72s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 4:  92%|█████████▏| 23/25 [04:20<00:22, 11.33s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 4:  96%|█████████▌| 24/25 [04:23<00:10, 10.96s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 4: 100%|██████████| 25/25 [04:25<00:00, 10.62s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 5:  80%|████████  | 20/25 [03:36<00:54, 10.82s/it, loss=2.03, v_num=mzjq]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  84%|████████▍ | 21/25 [04:12<00:48, 12.03s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 5:  88%|████████▊ | 22/25 [04:14<00:34, 11.59s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 5:  92%|█████████▏| 23/25 [04:17<00:22, 11.19s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 5:  96%|█████████▌| 24/25 [04:19<00:10, 10.83s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 5: 100%|██████████| 25/25 [04:22<00:00, 10.49s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 6:  80%|████████  | 20/25 [03:18<00:49,  9.93s/it, loss=2.03, v_num=mzjq]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  84%|████████▍ | 21/25 [03:53<00:44, 11.13s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 6:  88%|████████▊ | 22/25 [03:56<00:32, 10.74s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 6:  92%|█████████▏| 23/25 [03:58<00:20, 10.37s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 6:  96%|█████████▌| 24/25 [04:00<00:10, 10.04s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 6: 100%|██████████| 25/25 [04:03<00:00,  9.74s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 7:  80%|████████  | 20/25 [03:19<00:49,  9.97s/it, loss=2.03, v_num=mzjq]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  84%|████████▍ | 21/25 [03:55<00:44, 11.21s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 7:  88%|████████▊ | 22/25 [03:57<00:32, 10.82s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 7:  92%|█████████▏| 23/25 [04:00<00:20, 10.45s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 7:  96%|█████████▌| 24/25 [04:02<00:10, 10.12s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 7: 100%|██████████| 25/25 [04:05<00:00,  9.81s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 8:  80%|████████  | 20/25 [03:19<00:49,  9.97s/it, loss=2.03, v_num=mzjq]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8:  84%|████████▍ | 21/25 [03:55<00:44, 11.19s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 8:  88%|████████▊ | 22/25 [03:57<00:32, 10.79s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 8:  92%|█████████▏| 23/25 [04:00<00:20, 10.44s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 8:  96%|█████████▌| 24/25 [04:03<00:10, 10.13s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 8: 100%|██████████| 25/25 [04:05<00:00,  9.83s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 9:  80%|████████  | 20/25 [03:22<00:50, 10.14s/it, loss=2.03, v_num=mzjq]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9:  84%|████████▍ | 21/25 [03:57<00:45, 11.32s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 9:  88%|████████▊ | 22/25 [04:00<00:32, 10.91s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 9:  92%|█████████▏| 23/25 [04:02<00:21, 10.54s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 9:  96%|█████████▌| 24/25 [04:04<00:10, 10.20s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 9: 100%|██████████| 25/25 [04:07<00:00,  9.89s/it, loss=2.03, v_num=mzjq]\n",
      "Epoch 9: 100%|██████████| 25/25 [04:07<00:00,  9.89s/it, loss=2.03, v_num=mzjq]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 25/25 [04:07<00:00,  9.90s/it, loss=2.03, v_num=mzjq]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import torch.nn as nn\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from torchlightning_module import torch_lightning_DataModule\n",
    "from dataset_module import DatasetModule\n",
    "\n",
    "metadata_path='./metadata/flair-one_TOY_metadata.json'\n",
    "dataset_path='dataset'\n",
    "#df_for_split_logic='img_ids.jsonl'\n",
    "\n",
    "#ds=DatasetModule(metadata_path,dataset_path,train=False)\n",
    "\n",
    "###\n",
    "class SemanticSegmentationModel(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        for key in hparams.keys():\n",
    "             self.hparams[key]=hparams[key]\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.model = smp.FPN( #modifiy in-channels and output. \n",
    "           encoder_name=self.hparams['encoder_name'], \n",
    "           classes=self.hparams['classes'], \n",
    "           activation=self.hparams['activation'], \n",
    "           encoder_weights=self.hparams['encoder_weights'],\n",
    "            in_channels=self.hparams['in_channels'],\n",
    "\n",
    "              )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self.model(images)\n",
    "        labels = labels.squeeze(1).long() \n",
    "        loss = self.loss_fn(outputs, labels)\n",
    "        \n",
    "        \n",
    "        #outputs : output of model > logits /// labels > y ///images > x \n",
    "        self.log('train_loss', on_step=False, loss, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "#     def training_epoch_end(self, outputs):\n",
    "#         avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "#         self.log(\"avg_train_loss\", avg_loss)\n",
    "#         wandb.log({\"avg_train_loss\": avg_loss})\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self.model(images)\n",
    "        labels = labels.squeeze(1).long()\n",
    "        #print(f'{labels.shape} is shape of label in the val step method')\n",
    "        #print(f'{outputs.shape} is shape of outputs in the val step method')\n",
    "\n",
    "        loss = self.loss_fn(outputs, labels)\n",
    "        # Log loss and accuracy\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True)\n",
    "        \n",
    "        return {'val_loss': loss}\n",
    "\n",
    "#     def validation_epoch_end(self, outputs):\n",
    "#         avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "#         logs = {'val_loss': avg_loss}\n",
    "#         return {'val_loss': avg_loss, 'log': logs}\n",
    "\n",
    "        # def validation_epoch_end(self, outputs):\n",
    "        #     avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        #     logs = {'val_loss': avg_loss}\n",
    "        #     self.log('val_loss', avg_loss)\n",
    "\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "    \n",
    "# def lr_schedule(step):\n",
    "#    lr = 0.001\n",
    "#    if step < 10:\n",
    "#        return lr\n",
    "#    elif step < 20:\n",
    "#        return lr / 2\n",
    "#    else:\n",
    "#        return lr / 4\n",
    "\n",
    "#lr_scheduler = pl.callbacks.LearningRateScheduler(lr_schedule)\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "\n",
    "# Define your hyperparameters\n",
    "hparams = {\n",
    "    'encoder_name': 'efficientnet-b0', #resnet50\n",
    "    'classes': 19,\n",
    "    'activation': 'softmax',\n",
    "    'encoder_weights': 'imagenet', #imagenet\n",
    "    'lr': 0.001,#    'batch_size': 8,\n",
    "    'num_workers': 8,\n",
    "    'in_channels':5,\n",
    "\n",
    "}\n",
    "\n",
    "# Initialize your model\n",
    "model = SemanticSegmentationModel(hparams)\n",
    "data=torch_lightning_DataModule(batch_size=32,num_workers=8)\n",
    "\n",
    "wandb_logger = WandbLogger(project='phase2_semantic_segmentation_initial_run')\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10,# accelerator='gpu',\n",
    "                     callbacks=[early_stopping],\n",
    "                     logger=wandb_logger\n",
    "                    ) #lr_scheduler\n",
    "\n",
    "trainer.fit(model,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733e92b6-ff0a-49a5-95d4-ee0a9140a028",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
