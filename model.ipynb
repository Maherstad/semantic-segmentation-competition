{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ebd439-b1ce-4e18-929c-41bcca02849a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarkalsa\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20230214_114253-pkaq3y1v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/markalsa/phase2_semantic_segmentation_initial_run/runs/pkaq3y1v' target=\"_blank\">warm-violet-70</a></strong> to <a href='https://wandb.ai/markalsa/phase2_semantic_segmentation_initial_run' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/markalsa/phase2_semantic_segmentation_initial_run' target=\"_blank\">https://wandb.ai/markalsa/phase2_semantic_segmentation_initial_run</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/markalsa/phase2_semantic_segmentation_initial_run/runs/pkaq3y1v' target=\"_blank\">https://wandb.ai/markalsa/phase2_semantic_segmentation_initial_run/runs/pkaq3y1v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type             | Params\n",
      "---------------------------------------------\n",
      "0 | loss_fn | CrossEntropyLoss | 0     \n",
      "1 | model   | Unet             | 14.3 M\n",
      "---------------------------------------------\n",
      "14.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "14.3 M    Total params\n",
      "57.345    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/venv/lib/python3.9/site-packages/segmentation_models_pytorch/base/modules.py:116: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.activation(x)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab5f41772c74515bd122585f688cf84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import torch.nn as nn\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from torchlightning_module import torch_lightning_DataModule\n",
    "from dataset_module import DatasetModule\n",
    "\n",
    "#torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "###\n",
    "class SemanticSegmentationModel(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        for key in hparams.keys():\n",
    "             self.hparams[key]=hparams[key]\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.model = smp.Unet( #modifiy in-channels and output. \n",
    "           encoder_name=self.hparams['encoder_name'], \n",
    "           classes=self.hparams['classes'], \n",
    "           activation=self.hparams['activation'], \n",
    "           encoder_weights=self.hparams['encoder_weights'],\n",
    "            in_channels=self.hparams['in_channels'],\n",
    "\n",
    "              )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self.model(images)\n",
    "        labels = labels.squeeze(1)\n",
    "        loss = self.loss_fn(outputs, labels)\n",
    "        \n",
    "        \n",
    "        #outputs : output of model > logits /// labels > y ///images > x \n",
    "        self.log('train_loss',loss, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "#     def training_epoch_end(self, outputs):\n",
    "#         avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "#         self.log(\"avg_train_loss\", avg_loss)\n",
    "#         wandb.log({\"avg_train_loss\": avg_loss})\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self.model(images)\n",
    "        labels = labels.squeeze(1)\n",
    "        #print(f'{labels.shape} is shape of label in the val step method')\n",
    "        #print(f'{outputs.shape} is shape of outputs in the val step method')\n",
    "\n",
    "        loss = self.loss_fn(outputs, labels)\n",
    "        # Log loss and accuracy\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True)\n",
    "        \n",
    "        return {'val_loss': loss}\n",
    "\n",
    "#     def validation_epoch_end(self, outputs):\n",
    "#         avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "#         logs = {'val_loss': avg_loss}\n",
    "#         return {'val_loss': avg_loss, 'log': logs}\n",
    "\n",
    "        # def validation_epoch_end(self, outputs):\n",
    "        #     avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        #     logs = {'val_loss': avg_loss}\n",
    "        #     self.log('val_loss', avg_loss)\n",
    "\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "    \n",
    "# def lr_schedule(step):\n",
    "#    lr = 0.001\n",
    "#    if step < 10:\n",
    "#        return lr\n",
    "#    elif step < 20:\n",
    "#        return lr / 2\n",
    "#    else:\n",
    "#        return lr / 4\n",
    "\n",
    "#lr_scheduler = pl.callbacks.LearningRateScheduler(lr_schedule)\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=500,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "\n",
    "# Define your hyperparameters\n",
    "hparams = {\n",
    "    'encoder_name': 'resnet18', #resnet34,50\n",
    "    'classes': 13,\n",
    "    'activation': 'softmax',\n",
    "    'encoder_weights': 'imagenet', #imagenet\n",
    "    'lr': 0.001,#  \n",
    "    'in_channels':5,\n",
    "\n",
    "}\n",
    "\n",
    "# Initialize your model\n",
    "model = SemanticSegmentationModel(hparams)\n",
    "data=torch_lightning_DataModule(batch_size=16,num_workers=24,pin_memory=True)\n",
    "\n",
    "wandb_logger = WandbLogger(project='phase2_semantic_segmentation_initial_run')\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=7,\n",
    "                     accelerator='gpu',\n",
    "                     callbacks=[early_stopping],\n",
    "                     logger=wandb_logger\n",
    "                    ) #lr_scheduler\n",
    "\n",
    "trainer.fit(model,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2d376e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fedf3733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117\n",
      "CUDA version:  11.7\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(\"CUDA version: \", torch.version.cuda)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
