{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5ebd439-b1ce-4e18-929c-41bcca02849a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarkalsa\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20230213_184441-paqk7tz2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/markalsa/phase2_semantic_segmentation_initial_run/runs/paqk7tz2' target=\"_blank\">charmed-totem-23</a></strong> to <a href='https://wandb.ai/markalsa/phase2_semantic_segmentation_initial_run' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/markalsa/phase2_semantic_segmentation_initial_run' target=\"_blank\">https://wandb.ai/markalsa/phase2_semantic_segmentation_initial_run</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/markalsa/phase2_semantic_segmentation_initial_run/runs/paqk7tz2' target=\"_blank\">https://wandb.ai/markalsa/phase2_semantic_segmentation_initial_run/runs/paqk7tz2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type             | Params\n",
      "---------------------------------------------\n",
      "0 | loss_fn | CrossEntropyLoss | 0     \n",
      "1 | model   | FPN              | 23.2 M\n",
      "---------------------------------------------\n",
      "23.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.2 M    Total params\n",
      "92.656    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  1.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/venv/lib/python3.9/site-packages/segmentation_models_pytorch/base/modules.py:116: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.activation(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/envs/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  71%|███████▏  | 5/7 [00:05<00:02,  1.13s/it, loss=2.22, v_num=7tz2]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  86%|████████▌ | 6/7 [00:06<00:01,  1.02s/it, loss=2.22, v_num=7tz2]\n",
      "Epoch 0: 100%|██████████| 7/7 [00:06<00:00,  1.10it/s, loss=2.22, v_num=7tz2]\n",
      "Epoch 1:  71%|███████▏  | 5/7 [00:05<00:02,  1.06s/it, loss=2.13, v_num=7tz2]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  86%|████████▌ | 6/7 [00:05<00:00,  1.03it/s, loss=2.13, v_num=7tz2]\n",
      "Epoch 1: 100%|██████████| 7/7 [00:06<00:00,  1.15it/s, loss=2.13, v_num=7tz2]\n",
      "Epoch 2:  71%|███████▏  | 5/7 [00:05<00:02,  1.06s/it, loss=2.1, v_num=7tz2] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  86%|████████▌ | 6/7 [00:05<00:00,  1.03it/s, loss=2.1, v_num=7tz2]\n",
      "Epoch 2: 100%|██████████| 7/7 [00:06<00:00,  1.15it/s, loss=2.1, v_num=7tz2]\n",
      "Epoch 3:  71%|███████▏  | 5/7 [00:05<00:02,  1.06s/it, loss=2.08, v_num=7tz2]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  86%|████████▌ | 6/7 [00:05<00:00,  1.03it/s, loss=2.08, v_num=7tz2]\n",
      "Epoch 3: 100%|██████████| 7/7 [00:06<00:00,  1.15it/s, loss=2.08, v_num=7tz2]\n",
      "Epoch 4:  71%|███████▏  | 5/7 [00:05<00:02,  1.10s/it, loss=2.03, v_num=7tz2]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  86%|████████▌ | 6/7 [00:06<00:01,  1.00s/it, loss=2.03, v_num=7tz2]\n",
      "Epoch 4: 100%|██████████| 7/7 [00:06<00:00,  1.12it/s, loss=2.03, v_num=7tz2]\n",
      "Epoch 5:  71%|███████▏  | 5/7 [00:05<00:02,  1.11s/it, loss=2.03, v_num=7tz2]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  86%|████████▌ | 6/7 [00:06<00:01,  1.02s/it, loss=2.03, v_num=7tz2]\n",
      "Epoch 5: 100%|██████████| 7/7 [00:06<00:00,  1.10it/s, loss=2.03, v_num=7tz2]\n",
      "Epoch 6:  71%|███████▏  | 5/7 [00:05<00:02,  1.10s/it, loss=2.03, v_num=7tz2]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  86%|████████▌ | 6/7 [00:05<00:00,  1.00it/s, loss=2.03, v_num=7tz2]\n",
      "Epoch 6: 100%|██████████| 7/7 [00:06<00:00,  1.13it/s, loss=2.03, v_num=7tz2]\n",
      "Epoch 7:  57%|█████▋    | 4/7 [00:04<00:03,  1.04s/it, loss=2.03, v_num=7tz2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import torch.nn as nn\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from torchlightning_module import torch_lightning_DataModule\n",
    "from dataset_module import DatasetModule\n",
    "\n",
    "#torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "###\n",
    "class SemanticSegmentationModel(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        for key in hparams.keys():\n",
    "             self.hparams[key]=hparams[key]\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.model = smp.FPN( #modifiy in-channels and output. \n",
    "           encoder_name=self.hparams['encoder_name'], \n",
    "           classes=self.hparams['classes'], \n",
    "           activation=self.hparams['activation'], \n",
    "           encoder_weights=self.hparams['encoder_weights'],\n",
    "            in_channels=self.hparams['in_channels'],\n",
    "\n",
    "              )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self.model(images)\n",
    "        labels = labels.squeeze(1).long() \n",
    "        loss = self.loss_fn(outputs, labels)\n",
    "        \n",
    "        \n",
    "        #outputs : output of model > logits /// labels > y ///images > x \n",
    "        self.log('train_loss',loss, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "#     def training_epoch_end(self, outputs):\n",
    "#         avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "#         self.log(\"avg_train_loss\", avg_loss)\n",
    "#         wandb.log({\"avg_train_loss\": avg_loss})\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self.model(images)\n",
    "        labels = labels.squeeze(1).long()\n",
    "        #print(f'{labels.shape} is shape of label in the val step method')\n",
    "        #print(f'{outputs.shape} is shape of outputs in the val step method')\n",
    "\n",
    "        loss = self.loss_fn(outputs, labels)\n",
    "        # Log loss and accuracy\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True)\n",
    "        \n",
    "        return {'val_loss': loss}\n",
    "\n",
    "#     def validation_epoch_end(self, outputs):\n",
    "#         avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "#         logs = {'val_loss': avg_loss}\n",
    "#         return {'val_loss': avg_loss, 'log': logs}\n",
    "\n",
    "        # def validation_epoch_end(self, outputs):\n",
    "        #     avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        #     logs = {'val_loss': avg_loss}\n",
    "        #     self.log('val_loss', avg_loss)\n",
    "\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "    \n",
    "# def lr_schedule(step):\n",
    "#    lr = 0.001\n",
    "#    if step < 10:\n",
    "#        return lr\n",
    "#    elif step < 20:\n",
    "#        return lr / 2\n",
    "#    else:\n",
    "#        return lr / 4\n",
    "\n",
    "#lr_scheduler = pl.callbacks.LearningRateScheduler(lr_schedule)\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "\n",
    "# Define your hyperparameters\n",
    "hparams = {\n",
    "    'encoder_name': 'resnet34', #resnet50\n",
    "    'classes': 19,\n",
    "    'activation': 'softmax',\n",
    "    'encoder_weights': 'imagenet', #imagenet\n",
    "    'lr': 0.001,#  \n",
    "    'in_channels':5,\n",
    "\n",
    "}\n",
    "\n",
    "# Initialize your model\n",
    "model = SemanticSegmentationModel(hparams)\n",
    "data=torch_lightning_DataModule()\n",
    "\n",
    "wandb_logger = WandbLogger(project='phase2_semantic_segmentation_initial_run')\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10,\n",
    "                     accelerator='gpu',\n",
    "                     callbacks=[early_stopping],\n",
    "                     logger=wandb_logger\n",
    "                    ) #lr_scheduler\n",
    "\n",
    "trainer.fit(model,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64667c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "num_workers = multiprocessing.cpu_count() - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "733e92b6-ff0a-49a5-95d4-ee0a9140a028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569154fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
